{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54530ad7-2dc7-4edf-8fef-2c210c13bd75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c1ce7e-ffda-4d7e-81ac-51e686426458",
   "metadata": {},
   "source": [
    "# Base Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fd506f-1687-45f8-8b38-9aa4f78f29e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  (p | beam.Create(range(1, 20))\n",
    "     | beam.Filter(lambda num: num % 2 == 0)\n",
    "     | beam.Map(print))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe545bf-b3cd-411e-b559-ab7788e11c67",
   "metadata": {},
   "source": [
    "# Custom Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4a5960-bfd9-4350-a085-7f74b3a3643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "beam_options = PipelineOptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6283bec4-592e-419d-8e7a-9c6e2e8d971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "class MyOptions(PipelineOptions):\n",
    "  @classmethod\n",
    "  def _add_argparse_args(cls, parser):\n",
    "    parser.add_argument(\n",
    "        '--input',\n",
    "        default='gs://dataflow-samples/shakespeare/kinglear.txt',\n",
    "        help='The file path for the input text to process.')\n",
    "    parser.add_argument(\n",
    "        '--output', required=True, help='The path prefix for output files.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8683cb43-af5e-4b27-8f24-92a24bad48a4",
   "metadata": {},
   "source": [
    "# Adding name to steps in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0751343b-867b-432e-9912-542d3bb0ab2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  (p | \"Create set of numbers from 1 to 20\" >> beam.Create(range(1, 20))\n",
    "     | \"Step 2\" >> beam.Filter(lambda num: num % 2 == 0)\n",
    "     | \"Step 3\" >>beam.Map(print))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62a875a-bd1b-4359-8bd4-2d47093155eb",
   "metadata": {},
   "source": [
    "# Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5c77eb-6280-430c-8a6c-7f1e009dedfe",
   "metadata": {},
   "source": [
    "## Filtering with a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f273fd-422a-4f08-9ca2-6a5f197db555",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "def is_perennial(plant):\n",
    "  return plant['duration'] == 'perennial'\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  perennials = (\n",
    "      p | 'Gardening plants' >> beam.Create([\n",
    "          {\n",
    "              'icon': '🍓', 'name': 'Strawberry', 'duration': 'perennial'\n",
    "          },\n",
    "          {\n",
    "              'icon': '🥕', 'name': 'Carrot', 'duration': 'biennial'\n",
    "          },\n",
    "          {\n",
    "              'icon': '🍆', 'name': 'Eggplant', 'duration': 'perennial'\n",
    "          },\n",
    "          {\n",
    "              'icon': '🍅', 'name': 'Tomato', 'duration': 'annual'\n",
    "          },\n",
    "          {\n",
    "              'icon': '🥔', 'name': 'Potato', 'duration': 'perennial'\n",
    "          },\n",
    "      ])\n",
    "      | 'Filter perennials' >> beam.Filter(is_perennial)\n",
    "      | beam.Map(print))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abff3c00-394d-4583-a6f0-6fc99d7e6a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filtering with a lambda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2a9df8-decf-4d05-bc1a-0f8a49e6377d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "def is_perennial(plant):\n",
    "  return plant['duration'] == 'perennial'\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  perennials = (\n",
    "      p | 'Gardening plants' >> beam.Create([\n",
    "          {\n",
    "              'icon': '🍓', 'name': 'Strawberry', 'duration': 'perennial'\n",
    "          },\n",
    "          {\n",
    "              'icon': '🥕', 'name': 'Carrot', 'duration': 'biennial'\n",
    "          },\n",
    "          {\n",
    "              'icon': '🍆', 'name': 'Eggplant', 'duration': 'perennial'\n",
    "          },\n",
    "          {\n",
    "              'icon': '🍅', 'name': 'Tomato', 'duration': 'annual'\n",
    "          },\n",
    "          {\n",
    "              'icon': '🥔', 'name': 'Potato', 'duration': 'perennial'\n",
    "          },\n",
    "      ])\n",
    "      | 'Filter perennials' >> beam.Filter(lambda item: item[\"duration\"] == \"perennial\")\n",
    "      | beam.Map(print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7454670-921b-44d4-8111-ff4bed69aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filtering with multiple arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb1bad2-36be-47e5-a389-eb9357d584f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "def has_duration(plant, duration):\n",
    "  return plant['duration'] == duration\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  perennials = (\n",
    "      p | 'Gardening plants' >> beam.Create([\n",
    "          {\n",
    "              'icon': '🍓', 'name': 'Strawberry', 'duration': 'perennial'\n",
    "          },\n",
    "          {\n",
    "              'icon': '🥕', 'name': 'Carrot', 'duration': 'biennial'\n",
    "          },\n",
    "          {\n",
    "              'icon': '🍆', 'name': 'Eggplant', 'duration': 'perennial'\n",
    "          },\n",
    "          {\n",
    "              'icon': '🍅', 'name': 'Tomato', 'duration': 'annual'\n",
    "          },\n",
    "          {\n",
    "              'icon': '🥔', 'name': 'Potato', 'duration': 'perennial'\n",
    "          },\n",
    "      ])\n",
    "      | 'Filter perennials' >> beam.Filter(has_duration, \"biennial\")\n",
    "      | beam.Map(print))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb4eeb-c5a4-43ee-aa56-c7b7b42a2dfa",
   "metadata": {},
   "source": [
    "## Filtering with side inputs as singletons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c58c7-ae66-4153-aa12-be3a4d2ba44d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# If the PCollection has a single value, such as the average from another computation,\n",
    "# passing the PCollection as a singleton accesses that value.\n",
    "# In this example, we pass a PCollection the value perennial as a singleton. \n",
    "# We then use that value to filter out perennials.\n",
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  perennial = p | 'Perennial' >> beam.Create(['perennial'])\n",
    "\n",
    "  perennials = (\n",
    "      p | 'Gardening plants' >> beam.Create([\n",
    "          {\n",
    "              'icon': '🍓', 'name': 'Strawberry', 'duration': 'perennial'\n",
    "          },\n",
    "          {\n",
    "              'icon': '🥕', 'name': 'Carrot', 'duration': 'biennial'\n",
    "          },\n",
    "          {\n",
    "              'icon': '🍆', 'name': 'Eggplant', 'duration': 'perennial'\n",
    "          },\n",
    "          {\n",
    "              'icon': '🍅', 'name': 'Tomato', 'duration': 'annual'\n",
    "          },\n",
    "          {\n",
    "              'icon': '🥔', 'name': 'Potato', 'duration': 'perennial'\n",
    "          },\n",
    "      ])\n",
    "      | 'Filter perennials' >> beam.Filter(\n",
    "          lambda plant,\n",
    "          duration: plant['duration'] == duration,\n",
    "          duration=beam.pvalue.AsSingleton(perennial),\n",
    "      )\n",
    "      | beam.Map(print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e617df-851d-4ae7-9601-099a0843e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering with side input as iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39362c28-6252-41f6-8e3c-25cb67d87943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the PCollection has multiple values, pass the PCollection as an iterator.\n",
    "# This accesses elements lazily as they are needed, so it is possible to iterate\n",
    "# over large PCollections that won’t fit into memory.\n",
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  valid_durations = p | 'Valid durations' >> beam.Create([\n",
    "      'annual',\n",
    "      'biennial',\n",
    "      'perennial',\n",
    "  ])\n",
    "\n",
    "  valid_plants = (\n",
    "      p | 'Gardening plants' >> beam.Create([\n",
    "          {\n",
    "              'icon': '🍓', 'name': 'Strawberry', 'duration': 'perennial'\n",
    "          },\n",
    "          {\n",
    "              'icon': '🥕', 'name': 'Carrot', 'duration': 'biennial'\n",
    "          },\n",
    "          {\n",
    "              'icon': '🍆', 'name': 'Eggplant', 'duration': 'perennial'\n",
    "          },\n",
    "          {\n",
    "              'icon': '🍅', 'name': 'Tomato', 'duration': 'annual'\n",
    "          },\n",
    "          {\n",
    "              'icon': '🥔', 'name': 'Potato', 'duration': 'PERENNIAL'\n",
    "          },\n",
    "      ])\n",
    "      | 'Filter valid plants' >> beam.Filter(\n",
    "          lambda plant,\n",
    "          valid_durations: plant['duration'] in valid_durations,\n",
    "          valid_durations=beam.pvalue.AsIter(valid_durations),\n",
    "      )\n",
    "      | beam.Map(print))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff874c0a-f899-4ab4-91a2-56b249cdd540",
   "metadata": {},
   "source": [
    "## Filtering with side inputs as dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46237100-b69f-4c6c-8d50-27d9f1e72aed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If a PCollection is small enough to fit into memory, then that PCollection\n",
    "# can be passed as a dictionary. Each element must be a (key, value) pair.\n",
    "# Note that all the elements of the PCollection must fit into memory for this. \n",
    "# If the PCollection won’t fit into memory, use beam.pvalue.AsIter(pcollection) instead.\n",
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  keep_duration = p | 'Duration filters' >> beam.Create([\n",
    "      ('annual', False),\n",
    "      ('biennial', False),\n",
    "      ('perennial', True),\n",
    "  ])\n",
    "\n",
    "  perennials = (\n",
    "      p | 'Gardening plants' >> beam.Create([\n",
    "          {\n",
    "              'icon': '🍓', 'name': 'Strawberry', 'duration': 'perennial'\n",
    "          },\n",
    "          {\n",
    "              'icon': '🥕', 'name': 'Carrot', 'duration': 'biennial'\n",
    "          },\n",
    "          {\n",
    "              'icon': '🍆', 'name': 'Eggplant', 'duration': 'perennial'\n",
    "          },\n",
    "          {\n",
    "              'icon': '🍅', 'name': 'Tomato', 'duration': 'annual'\n",
    "          },\n",
    "          {\n",
    "              'icon': '🥔', 'name': 'Potato', 'duration': 'perennial'\n",
    "          },\n",
    "      ])\n",
    "      | 'Filter plants by duration' >> beam.Filter(\n",
    "          lambda plant,\n",
    "          keep_duration: keep_duration[plant['duration']],\n",
    "          keep_duration = beam.pvalue.AsDict(keep_duration),\n",
    "      )\n",
    "      | beam.Map(print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d73ca-65d3-449c-9077-edab40d8979a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import string\n",
    "\n",
    "\n",
    "def has_duration(plant):\n",
    "  return (\"A\" in plant)\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    str = \"To be, or not to be: that is the question: Whether 'tis nobler in the mind to suffer The slings and arrows of outrageous fortune, Or to take arms against a sea of troubles, And by opposing end them. To die: to sleep\"\n",
    "\n",
    "    input = (p | beam.Create([str.split()])\n",
    "             | beam.Filter(has_duration)\n",
    "            | beam.Map(print))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549f4f53-3beb-4814-a977-7625fb4dc1a6",
   "metadata": {},
   "source": [
    "# Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59e8bf4-f21d-4004-9395-47e2f72e9e09",
   "metadata": {},
   "source": [
    "## Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ead044-4dcf-4787-9f74-92875eee2350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "# Output PCollection\n",
    "class Output(beam.PTransform):\n",
    "    class _OutputFn(beam.DoFn):\n",
    "        def __init__(self, prefix=''):\n",
    "            super().__init__()\n",
    "            self.prefix = prefix\n",
    "\n",
    "        def process(self, element):\n",
    "            print(self.prefix+str(element))\n",
    "\n",
    "    def __init__(self, label=None,prefix=''):\n",
    "        super().__init__(label)\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def expand(self, input):\n",
    "        input | beam.ParDo(self._OutputFn(self.prefix))\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  (p | beam.Create(range(1, 14))\n",
    "   # beam.combiners.Count.Globally() to return the count of numbers from `PCollection`.\n",
    "   | beam.combiners.Count.Globally()\n",
    "   | \"Print\" >> Output(prefix='Input has elements:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f24e2d9-3543-4327-a9b5-694f1d64c0a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  total_elements = (\n",
    "      p | 'Create plants' >> beam.Create(['🍓', '🥕', '🥕', '🥕', '🍆', '🍆', '🍅', '🍅', '🍅', '🌽'])\n",
    "      | 'Count all elements' >> beam.combiners.Count.Globally()\n",
    "      | beam.Map(print))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113f8d37-54b5-41c0-a10f-654e3465cb8b",
   "metadata": {},
   "source": [
    "## Count by Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e9746a-9c9a-40af-89ee-7331f5187735",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  total_elements = (\n",
    "      p | 'Create plants' >> beam.Create([\n",
    "          ('spring', '🍓'),\n",
    "          ('spring', '🥕'),\n",
    "          ('summer', '🥕'),\n",
    "          ('fall', '🥕'),\n",
    "          ('spring', '🍆'),\n",
    "          ('winter', '🍆'),\n",
    "          ('spring', '🍅'),\n",
    "          ('summer', '🍅'),\n",
    "          ('fall', '🍅'),\n",
    "          ('summer', '🌽'),])\n",
    "      | 'Count all elements' >> beam.combiners.Count.PerKey()\n",
    "      | beam.Map(print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a8bfac-878e-49fe-a547-d028934d5029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "class SplitWords(beam.DoFn):\n",
    "  def __init__(self, delimiter=' '):\n",
    "    self.delimiter = delimiter\n",
    "\n",
    "  def process(self, text):\n",
    "    for word in text.split(self.delimiter):\n",
    "      yield word\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  total_unique_elements = (\n",
    "      p | beam.Create([\"To be, or not to be: that is the question: Whether 'tis nobler in the mind to suffer, the slings and arrows of outrageous fortune, or to take arms against a sea of troubles, and by opposing end them. To die: to sleep\"])\n",
    "  | beam.ParDo(SplitWords()) | beam.combiners.Count.PerElement()\n",
    "  | Output(prefix='PCollection filtered value: '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57a37c7-efa7-49f2-97b2-f04ee56c8fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count Unique keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a116526-0ebc-4d11-8ab2-b652843f1cac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  total_unique_elements = (\n",
    "      p | 'Create produce' >> beam.Create(['🍓', '🥕', '🥕', '🥕', '🍆', '🍆', '🍅', '🍅', '🍅', '🌽'])\n",
    "      | 'Count unique elements' >> beam.combiners.Count.PerElement()\n",
    "      | beam.Map(print))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3a8965-ee4d-41a8-83e9-c68cf2e3935f",
   "metadata": {},
   "source": [
    "## Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d698fd-6120-43ec-a9f3-479cd65beef6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  total = (\n",
    "      p | 'Create numbers' >> beam.Create([3, 4, 1, 2])\n",
    "      | 'Sum values' >> beam.CombineGlobally(sum)\n",
    "      | beam.Map(print))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e863201-35af-487b-96e0-a978887ecd55",
   "metadata": {},
   "source": [
    "## Sum per each Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef3fc61-01d4-4ffe-ac57-0e05186607a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  totals_per_key = (\n",
    "      p | 'Create produce' >> beam.Create([\n",
    "          ('🥕', 3),\n",
    "          ('🥕', 2),\n",
    "          ('🍆', 1),\n",
    "          ('🍅', 4),\n",
    "          ('🍅', 5),\n",
    "          ('🍅', 3),])\n",
    "      | 'Sum values per key' >> beam.CombinePerKey(sum)\n",
    "      | beam.Map(print))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e169d4-86a6-4337-975a-80769d96e5b1",
   "metadata": {},
   "source": [
    "## Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f861d0-9fec-4368-b07a-35fef35ec58e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  mean_per_key = (\n",
    "      p | 'Create produce' >> beam.Create([\n",
    "          (1, 36),(2, 91),(3, 33),(3, 11),(4, 67),])\n",
    "      | 'Mean' >> beam.combiners.Mean.PerKey()\n",
    "      | beam.Map(print))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3844b2-06d4-4006-b174-6a307899ecf9",
   "metadata": {},
   "source": [
    "## Min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c23e0-1bb2-4777-a1c9-8a6c61d694c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  min_element = (\n",
    "      p | 'Create numbers' >> beam.Create([3, 44,23,6767, 100, 2])\n",
    "      | 'Get min value' >> beam.CombineGlobally(lambda elements: min(elements or [-1]))\n",
    "      | beam.Map(print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c825c31-001e-4d2f-8ec2-f90f00cc3872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  elements_with_min_value_per_key = (\n",
    "      p | 'Create produce' >> beam.Create([\n",
    "          ('🥕', 3),\n",
    "          ('🥕', 2),\n",
    "          ('🍆', 1),\n",
    "          ('🍅', 4),\n",
    "          ('🍅', 5),\n",
    "          ('🍅', 3),])\n",
    "      | 'Get min value per key' >> beam.CombinePerKey(min)\n",
    "      | beam.Map(print))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf89227-f867-4aff-9ea6-ba6465968ad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  elements_with_min_value_per_key = (\n",
    "      p | beam.Create([(1, 360),(1, 91),(3, 33),(3, 11),(2, 67),])\n",
    "      | 'Get min value per key' >> beam.CombinePerKey(min)\n",
    "      | 'Top 2' >> beam.combiners.Top.Smallest(2)\n",
    "      | beam.Map(print))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1148c46-c04f-4ffb-8c2a-5fd901f5a70b",
   "metadata": {},
   "source": [
    "## Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f402c355-f873-4916-8946-faa20574e0bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  max_element = (\n",
    "      p | 'Create numbers' >> beam.Create([3, 4, 1, 2,9990])\n",
    "      | 'Get max value' >> beam.CombineGlobally(lambda elements: max(elements or [None]))\n",
    "      | beam.Map(print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb574f6-7880-4a03-a9c4-e161b52db8ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  elements_with_max_value_per_key = (\n",
    "      p | 'Create produce' >> beam.Create([\n",
    "          ('🥕', 3),\n",
    "          ('🥕', 2),\n",
    "          ('🍆', 1),\n",
    "          ('🍅', 4),\n",
    "          ('🍅', 5),\n",
    "          ('🍅', 3),])\n",
    "      | 'Get max value per key' >> beam.CombinePerKey(max)\n",
    "      | beam.Map(print))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102af902-e94e-4e8f-a417-65519b5777c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    (p | beam.Create([\n",
    "        (1, 36),\n",
    "        (2, 91),\n",
    "        (3, 33),\n",
    "        (3, 11),\n",
    "        (4, 67),\n",
    "        (2, 679),\n",
    "    ]) \n",
    "       # | beam.CombinePerKey(max)\n",
    "       | beam.combiners.Top.Largest(2)\n",
    "       | beam.Map(print) )\n",
    "# (1, 36)\n",
    "# (2, 679)\n",
    "# (3, 33)\n",
    "# (4, 67)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b5dd61-e1b7-4a95-9f42-5882f50b5d30",
   "metadata": {},
   "source": [
    "# WithKey Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c09ac9d6-5150-40df-bdd3-24212e444783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('apple', 'apple')\n",
      "('banana', 'banana')\n",
      "('cherry', 'cherry')\n",
      "('durian', 'durian')\n",
      "('guava', 'guava')\n",
      "('melon', 'melon')\n",
      "('banana', 'banana')\n"
     ]
    }
   ],
   "source": [
    "# WithKeys takes a PCollection<V> and produces a PCollection<KV<K, V>> by associating each\n",
    "# input element with a key.\n",
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  (p | beam.Create(['apple', 'banana', 'cherry', 'durian', 'guava', 'melon', 'banana'])\n",
    "     | beam.WithKeys(lambda word: word)\n",
    "     | beam.Map(print))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af60d78-c31e-4e80-9bd3-69654e616b95",
   "metadata": {},
   "source": [
    "# Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce3288f-095f-4054-96df-ffed54768755",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "def parse_csv_line(line):\n",
    "    # Split the line by comma (adjust delimiter if needed)\n",
    "    fields = line.split(',')\n",
    "    # Assuming headers exist in the first line, skip the header row\n",
    "    # if line.startswith('VendorID,tpep_dropoff_datetime,passenger_count,trip_distance,RatecodeID,store_and_fwd_flag,PULocationID,DOLocationID,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount'):\n",
    "    if fields[16] == '\"total_amount\"':\n",
    "        yield None\n",
    "    else:        \n",
    "          # Extract data from each field based on your schema\n",
    "        yield float(fields[16])  # Assuming field2 is numerical\n",
    "              # ... extract other fields based on their data types\n",
    "          # }\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "  # Read the CSV file line by line\n",
    "    lines = ( pipeline | beam.io.ReadFromText('/home/sanvir/Downloads/sample1000.csv'))\n",
    "\n",
    "    parsed_lines = (lines | beam.ParDo(parse_csv_line))\n",
    "    # Apply a ParDo transform to parse each line\n",
    "    above = (parsed_lines\n",
    "            | beam.Filter(lambda cost: cost and cost >= 15)\n",
    "            | \"sum above\" >> beam.CombineGlobally(sum) \n",
    "            | beam.WithKeys(lambda word: \"above\")\n",
    "            | \"print above\" >> beam.Map(print))\n",
    "    \n",
    "    below = (parsed_lines\n",
    "            | beam.Filter(lambda cost: cost and cost < 15)\n",
    "            | \"sum below\" >> beam.CombineGlobally(sum) \n",
    "            | beam.WithKeys(lambda word: \"below\")\n",
    "            | beam.Map(print))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43390ee7-9dbb-40b3-96f2-44b668efed30",
   "metadata": {},
   "source": [
    "# Core Beam transforms\n",
    "Beam provides the following core transforms, each of which represents a different processing paradigm:\n",
    "\n",
    "- ParDo\n",
    "- GroupByKey\n",
    "- CoGroupByKey\n",
    "- Combine\n",
    "- Flatten\n",
    "- Partition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e3684a-289a-4d83-8d2c-40f725449564",
   "metadata": {},
   "source": [
    "# ParDo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1013d16a-e62f-493a-bf2c-acb13d735c32",
   "metadata": {},
   "source": [
    "## one to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5af97e9-9809-43f4-bed9-1083c6cbffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "class MultiplyByTenDoFn(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        yield element * 10\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  (p | beam.Create([1, 2, 3, 4, 5])\n",
    "    # Transform simple DoFn operation\n",
    "     | beam.ParDo(MultiplyByTenDoFn())\n",
    "     | beam.Map(print))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9feaa8-07c2-4d2a-a841-acdf4cf9d4ff",
   "metadata": {},
   "source": [
    "## one to many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d11a85d-71cd-467d-8d42-22d02710d02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Beam\n",
      "It\n",
      "is\n",
      "awesome\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "class BreakIntoWordsDoFn(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        return element.split()\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  (p | beam.Create(['Hello Beam', 'It is awesome'])\n",
    "     | beam.ParDo(BreakIntoWordsDoFn())\n",
    "     | beam.Map(print))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01551dc-db43-41a9-a273-75e4bff6bcc1",
   "metadata": {},
   "source": [
    "## Map elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65b058e-42b8-4f6d-9e99-f1023b889310",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    input = p | 'Create words' >> beam.Create(['Hello', 'World', 'How', 'are', 'you'])\n",
    "\n",
    "    uppercase_words = input | 'Convert to uppercase' >> beam.Map(lambda word: word.upper())\n",
    "\n",
    "    (uppercase_words | beam.Map(print))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c43282-f43d-4f05-8674-5751cd5065cd",
   "metadata": {},
   "source": [
    "## FlatMap elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afd48d9-f060-4973-9990-7d1703d9d5d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    words_with_counts = p | 'Create words with counts' >> beam.Create([\n",
    "    ('Hello', 1), ('World', 2), ('How', 3), ('are', 4), ('you', 5)])\n",
    "\n",
    "    split_words = words_with_counts | 'Split words' >> beam.FlatMap(\n",
    "    lambda word_with_count: [word_with_count[0]] * word_with_count[1])\n",
    "    \n",
    "    (split_words | beam.Map(print))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737e074d-6ba5-486d-8e1f-9182ff9e6a38",
   "metadata": {},
   "source": [
    "## GroupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb909374-a043-4713-ba87-353502661cd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('banana', [2, 5])\n",
      "('apple', [4, 1])\n",
      "('lemon', [3, 12])\n",
      "('datyra', [21, 22])\n",
      "('sanvir', [20])\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "    input = p | 'Fruits' >> beam.Create([\n",
    "        (\"banana\", 2),\n",
    "        (\"apple\", 4),\n",
    "        (\"lemon\", 3),\n",
    "        (\"Apple\", 1),\n",
    "        (\"Banana\", 5),\n",
    "        (\"Lemon\", 12),\n",
    "        (\"Datyra\", 21),\n",
    "        (\"datyra\", 22),\n",
    "        (\"sanvir\", 20)\n",
    "    ])\n",
    "    \n",
    "    (input \n",
    "     | beam.Map(lambda data: (data[0].lower(), data[1]) )\n",
    "     | beam.GroupByKey()\n",
    "     | beam.Map(print))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158a6698-d39e-4ec9-8453-ee18a40de955",
   "metadata": {},
   "source": [
    "## CoGroupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca50630-0693-4c1f-a891-28f3223565c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CoGroupByKey is a transformation used in Apache Beam for performing joins on multiple datasets\n",
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "# // Mock data\n",
    "emails_list = [\n",
    "    ('amy', 'amy@example.com'),\n",
    "    ('carl', 'carl@example.com'),\n",
    "    ('julia', 'julia@example.com'),\n",
    "    ('carl', 'carl@email.com'),\n",
    "]\n",
    "phones_list = [\n",
    "    ('amy', '111-222-3333'),\n",
    "    ('james', '222-333-4444'),\n",
    "    ('amy', '333-444-5555'),\n",
    "    ('carl', '444-555-6666'),\n",
    "]\n",
    "\n",
    "def join_info(name_info):\n",
    "  (name, info) = name_info\n",
    "  return '%s; %s; %s' %\\\n",
    "      (name, sorted(info['emails']), sorted(info['phones']))\n",
    "\n",
    "# // Creating PCollections\n",
    "with beam.Pipeline() as p:\n",
    "    emails = p | 'CreateEmails' >> beam.Create(emails_list)\n",
    "    phones = p | 'CreatePhones' >> beam.Create(phones_list)\n",
    "\n",
    "    # // Apply CoGroupByKey\n",
    "    results = ({'emails': emails, 'phones': phones} | beam.CoGroupByKey())\n",
    "\n",
    "    # contact_lines = results | beam.Map(join_info)\n",
    "    \n",
    "    (results | beam.Map(print))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbb60be-7a6f-4fe0-a028-fb95f9f7a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tags for multiple outputs (withOutputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58788cbf-3ff0-428d-8f08-ff1db0661d94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "# To emit elements to multiple output PCollections, invoke with_outputs() on the ParDo, and specify the\n",
    "# expected tags for the outputs. with_outputs() returns a DoOutputsTuple object. Tags specified in\n",
    "# with_outputs are attributes on the returned DoOutputsTuple object. The tags give access to the\n",
    "# corresponding output PCollections.\n",
    "\n",
    "# The result is also iterable, ordered in the same order that the tags were passed to with_outputs(),\n",
    "# the main tag (if specified) first.\n",
    "\n",
    "class ProcessWords(beam.DoFn):\n",
    "    def process(self, element, cutoff_length, marker):\n",
    "        if len(element) <= cutoff_length:\n",
    "            # Emit this short word to the main output.\n",
    "            yield element\n",
    "        else:\n",
    "            # Emit this word's long length to the 'above_cutoff_lengths' output.\n",
    "            yield beam.pvalue.TaggedOutput('above_cutoff_lengths', len(element))\n",
    "        if element.startswith(marker):\n",
    "            # Emit this word to a different output with the 'marked strings' tag.\n",
    "            yield beam.pvalue.TaggedOutput('marked strings', element)\n",
    "\n",
    "with beam.Pipeline() as p:   \n",
    "    words = (p | beam.Create([\"em\",\"elements\",\"multiple\",\"output\",\"xico\"]))\n",
    "    below, above, marked = (words | beam.ParDo(ProcessWords(), cutoff_length=2, marker='x')\n",
    "                                .with_outputs('above_cutoff_lengths','marked strings',main='below_cutoff_strings'))\n",
    "\n",
    "    (above | 'Above'>> beam.Map(print))\n",
    "    \n",
    "    (below | 'Below'>> beam.Map(print))\n",
    "    \n",
    "    (marked | 'Marked'>> beam.Map(print))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23a9fee-650b-4fe4-b16e-1271e1f28186",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "# Producing multiple outputs is also available in Map and FlatMap.\n",
    "# Here is an example that uses FlatMap and shows that the tags do not need to be specified ahead of time.\n",
    "def even_odd(x):\n",
    "  yield beam.pvalue.TaggedOutput('odd' if x % 2 else 'even', x)\n",
    "  if x % 10 == 0:\n",
    "    yield x\n",
    "\n",
    "with beam.Pipeline() as p: \n",
    "    results = ( p |  beam.Create(range(10))\n",
    "        | beam.FlatMap(even_odd).with_outputs() )\n",
    "\n",
    "    evens = results.even\n",
    "    odds = results.odd\n",
    "    tens = results[None]  # the undeclared main output\n",
    "\n",
    "    (evens | 'Above'>> beam.Map(print))\n",
    "    \n",
    "    # (odds | 'Below'>> beam.Map(print))\n",
    "#     \n",
    "    # (tens | 'Marked'>> beam.Map(print))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a01f56b-45de-4090-88a6-89d6992e54fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Branching PCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad3ca66-4b59-416a-915a-95957f8ac6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example how can we move the logic to an function\n",
    "func applyTransform(s beam.Scope, input beam.PCollection) (beam.PCollection, beam.PCollection) {\n",
    "    return beam.ParDo2(s, func(element string, upperCaseWords, lowerCaseWords func(string)) {\n",
    "        if element==strings.Title(element) {\n",
    "                upperCaseWords(element)\n",
    "                return\n",
    "            }\n",
    "            lowerCaseWords(element)\n",
    "        }, input)\n",
    "}\n",
    "\n",
    "# Sample of Branching\n",
    "reversed = input | reverseString(...)\n",
    "toUpper = input | toUpperString(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3707697e-e901-4097-a83f-e37b2c8f55bb",
   "metadata": {},
   "source": [
    "## Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c6c88-86c2-48ed-8793-5da4a2bcf521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "# Is a Beam transform for combining collections of elements or values in your data. \n",
    "# Combine has variants that work on entire PCollections, and some that combine the values for each key in PCollections of key/value pairs.\n",
    "\n",
    "input = [1, 10, 100, 1000]\n",
    "\n",
    "def bounded_sum(values, bound=500):\n",
    "  return min(sum(values), bound)\n",
    "\n",
    "with beam.Pipeline() as p: \n",
    "    small_sum = input | beam.CombineGlobally(bounded_sum)  # [500]\n",
    "    large_sum = input | beam.CombineGlobally(bounded_sum, bound=5000)\n",
    "    \n",
    "    (small_sum | beam.Map(print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e532cb-f21b-4797-a8bd-880c5e77e05b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "# Output PCollection\n",
    "class Output(beam.PTransform):\n",
    "    class _OutputFn(beam.DoFn):\n",
    "        def __init__(self, prefix=''):\n",
    "            super().__init__()\n",
    "            self.prefix = prefix\n",
    "\n",
    "        def process(self, element):\n",
    "            print(self.prefix+str(element))\n",
    "\n",
    "    def __init__(self, label=None,prefix=''):\n",
    "        super().__init__(label)\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def expand(self, input):\n",
    "        input | beam.ParDo(self._OutputFn(self.prefix))\n",
    "\n",
    "def concat(strings):\n",
    "    total = \"\"\n",
    "\n",
    "    for word in strings:\n",
    "        total += word\n",
    "\n",
    "    return total\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  (p | beam.Create([\"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"])\n",
    "     | beam.CombineGlobally(concat)\n",
    "     | Output())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9bcecb-fec5-4fa9-93fb-71227acc1ac7",
   "metadata": {},
   "source": [
    "## CombineFn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901bda90-4f83-487c-83e0-d59aaf11f127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is a class that specifies how to combine a collection of elements into a single output value. \n",
    "# It's a powerful tool for performing aggregations and summarizing data within your Beam pipelines.\n",
    "\n",
    "\n",
    "# It defines three methods:\n",
    "\n",
    "# createAccumulator(): Creates a new accumulator to hold intermediate results. This is called for each partition (batch) of data.\n",
    "# addInput(AccumT accumulator, InputT input): Combines a single input element with the current accumulator, updating the accumulator to reflect the new information.\n",
    "# mergeAccumulators(Iterable<AccumT> accumulators): Merges multiple accumulators into a single accumulator. This is useful when processing data in parallel.\n",
    "# (Optional) extractOutput(AccumT accumulator): Transforms the final accumulator into the desired output format.\n",
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "# Output PCollection\n",
    "class Output(beam.PTransform):\n",
    "    class _OutputFn(beam.DoFn):\n",
    "        def __init__(self, prefix=''):\n",
    "            super().__init__()\n",
    "            self.prefix = prefix\n",
    "\n",
    "        def process(self, element):\n",
    "            print(self.prefix+str(element))\n",
    "\n",
    "    def __init__(self, label=None,prefix=''):\n",
    "        super().__init__(label)\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def expand(self, input):\n",
    "        input | beam.ParDo(self._OutputFn(self.prefix))\n",
    "\n",
    "class AverageFn(beam.CombineFn):\n",
    "\n",
    "    def create_accumulator(self):\n",
    "        return 0.0, 0\n",
    "\n",
    "    def add_input(self, accumulator, element):\n",
    "        (sum, count) = accumulator\n",
    "        return sum + element, count + 1\n",
    "\n",
    "    def merge_accumulators(self, accumulators):\n",
    "        sums, counts = zip(*accumulators)\n",
    "        return sum(sums), sum(counts)\n",
    "\n",
    "    def extract_output(self, accumulator):\n",
    "        (sum, count) = accumulator\n",
    "        return sum / count if count else float('NaN')\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  (p | beam.Create([10, 20, 50, 70, 90, 90000])\n",
    "     | beam.CombineGlobally(AverageFn())\n",
    "     | Output())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc51e56-c914-46f2-83a6-42a4c88e4350",
   "metadata": {},
   "source": [
    "## Combine-per-key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8289052-a60c-467c-bcc7-5420b7432353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It defines three methods:\n",
    "\n",
    "#     createAccumulator(): Creates a new accumulator to hold intermediate results. This is called for each partition (batch) of data.\n",
    "#     addInput(AccumT accumulator, InputT input): Combines a single input element with the current accumulator, updating the accumulator to reflect the new information.\n",
    "#     mergeAccumulators(Iterable<AccumT> accumulators): Merges multiple accumulators into a single accumulator. This is useful when processing data in parallel.\n",
    "#     (Optional) extractOutput(AccumT accumulator): Transforms the final accumulator into the desired output format.\n",
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    input_data = [('a', 1), ('b', 2), ('a', 3), ('c', 4), ('b', 5)]\n",
    "    input_pcoll = p | beam.Create(input_data)\n",
    "    output_pcoll = input_pcoll | beam.CombinePerKey(sum)\n",
    "\n",
    "    (output_pcoll | beam.Map(print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9d92630-2e92-47c3-af3b-7233b3d1e6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', ',apple,avocado')\n",
      "('o', ',orange')\n",
      "('l', ',lemon,limes')\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "class ConcatString(beam.CombineFn):\n",
    "    def create_accumulator(self):\n",
    "        return \"\"\n",
    "\n",
    "    def add_input(self, accumulator, input):\n",
    "        return accumulator + \",\" + input\n",
    "\n",
    "    def merge_accumulators(self, accumulators):\n",
    "        return ''.join(accumulators)\n",
    "\n",
    "    def extract_output(self, accumulator):\n",
    "        return accumulator\n",
    "\n",
    "with beam.Pipeline() as p:    \n",
    "    input = (p | 'Create Cities To Time KV' >> beam.Create([\n",
    "             ('a', 'apple'),\n",
    "             ('o', 'orange'),\n",
    "             ('a', 'avocado'),\n",
    "             ('l', 'lemon'),\n",
    "             ('l', 'limes')])\n",
    "    )\n",
    "\n",
    "    output = input | 'Combine Per Key' >> beam.CombinePerKey(ConcatString())\n",
    "\n",
    "    (output | beam.Map(print) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80b7061-a285-4878-857a-4649052684bd",
   "metadata": {},
   "source": [
    "# Composite Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1aaf85-cd02-406f-b74d-b6cdcb88c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More than one ParDo, Combine, GroupByKey, or even other composite transforms.\n",
    "# These transforms are called composite transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d34bc-9dc6-46ca-8400-cc9c6e26757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "# Output PCollection\n",
    "class Output(beam.PTransform):\n",
    "    class _OutputFn(beam.DoFn):\n",
    "        def __init__(self, prefix=''):\n",
    "            super().__init__()\n",
    "            self.prefix = prefix\n",
    "\n",
    "        def process(self, element):\n",
    "            print(self.prefix+str(element))\n",
    "\n",
    "    def __init__(self, label=None,prefix=''):\n",
    "        super().__init__(label)\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def expand(self, input):\n",
    "        input | beam.ParDo(self._OutputFn(self.prefix))\n",
    "\n",
    "class ExtractAndMultiplyNumbers(beam.PTransform):\n",
    "    def expand(self, pcoll):\n",
    "        return (pcoll\n",
    "                # First operation\n",
    "                | beam.FlatMap(lambda line: map(int, line.split(',')))\n",
    "                # Second operation\n",
    "                | beam.Map(lambda num: num * 10)\n",
    "                )\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  (p | beam.Create(['1,2,3,4,5', '6,7,8,9,10']) \\\n",
    "     | ExtractAndMultiplyNumbers() \\\n",
    "     | Output())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7857965-d411-4ffb-880b-60b7f8ad16b6",
   "metadata": {},
   "source": [
    "# Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0307a5bc-908c-46c9-a5e5-641457bafbfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple\n",
      "ant\n",
      "arrow\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "ball\n",
      "book\n",
      "bow\n"
     ]
    }
   ],
   "source": [
    "# Flatten is a Beam transform for PCollection objects that store the same data type.\n",
    "# Flatten merges multiple PCollection objects into a single logical PCollection.\n",
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "# Output PCollection\n",
    "class Output(beam.PTransform):\n",
    "    class _OutputFn(beam.DoFn):\n",
    "        def __init__(self, prefix=''):\n",
    "            super().__init__()\n",
    "            self.prefix = prefix\n",
    "\n",
    "        def process(self, element):\n",
    "            print(self.prefix+str(element))\n",
    "\n",
    "    def __init__(self, label=None,prefix=''):\n",
    "        super().__init__(label)\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def expand(self, input):\n",
    "        input | beam.ParDo(self._OutputFn(self.prefix))\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  # List of elements start with a\n",
    "  wordsStartingWithA = p | 'Words starting with A' >> beam.Create(['apple', 'ant', 'arrow'])\n",
    "\n",
    "  # List of elements start with b\n",
    "  wordsStartingWithB = p | 'Words starting with B' >> beam.Create(['ball','book', 'bow'])\n",
    "\n",
    "  test = p | 'Test' >> beam.Create(range(20))\n",
    "    \n",
    "  # Accept two PCollection data types are the same combines and returns one PCollection\n",
    "  ((wordsStartingWithA, wordsStartingWithB, test) | beam.Flatten() | Output())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1cb280-82ca-4ceb-8fce-290f738247b3",
   "metadata": {},
   "source": [
    "# Pertition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67f4fe-ffc8-4e83-92fa-00992cbe6ace",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Partition is a Beam transform for PCollection objects that store the same data type. \n",
    "# Partition splits a single PCollection into a fixed number of smaller collections.\n",
    "import apache_beam as beam\n",
    "\n",
    "def partition_fn(number, num_partitions):\n",
    "    if number > 100:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  results = (p | beam.Create([1, 2, 3, 4, 5, 100, 110, 150, 250])\n",
    "        # Accepts PCollection and returns the PCollection array\n",
    "         | beam.Partition(partition_fn, 2))\n",
    "\n",
    "  results[0] | 'Log numbers > 100' >> beam.Map(print)\n",
    "  results[1] | 'Log numbers <= 100' >> beam.Map(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c2fbc8-89c1-4a24-bfa4-20fc00e53ebc",
   "metadata": {},
   "source": [
    "# Side inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d649a5c2-caf7-4dae-a1e8-38a808842445",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person[Henry,Singapore,Singapore]\n",
      "Person[Jane,San Francisco,United States]\n",
      "Person[Lee,Beijing,China]\n",
      "Person[John,Sydney,Australia]\n",
      "Person[Alfred,London,United Kingdom]\n"
     ]
    }
   ],
   "source": [
    "# You can provide additional inputs to a ParDo transform in the form of side inputs.\n",
    "import apache_beam as beam\n",
    "\n",
    "class Person:\n",
    "    def __init__(self, name, city, country=''):\n",
    "        self.name = name\n",
    "        self.city = city\n",
    "        self.country = country\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Person[' + self.name + ',' + self.city + ',' + self.country + ']'\n",
    "\n",
    "\n",
    "class EnrichCountryDoFn(beam.DoFn):\n",
    "    # Get city from cities_to_countries and set person\n",
    "    def process(self, element, cities_to_countries):\n",
    "        yield Person(element.name, element.city,\n",
    "                     cities_to_countries[element.city])\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  # List of elements\n",
    "  cities_to_countries = {\n",
    "        'Beijing': 'China',\n",
    "        'London': 'United Kingdom',\n",
    "        'San Francisco': 'United States',\n",
    "        'Singapore': 'Singapore',\n",
    "        'Sydney': 'Australia'\n",
    "    }\n",
    "\n",
    "  persons = [\n",
    "        Person('Henry', 'Singapore'),\n",
    "        Person('Jane', 'San Francisco'),\n",
    "        Person('Lee', 'Beijing'),\n",
    "        Person('John', 'Sydney'),\n",
    "        Person('Alfred', 'London')\n",
    "    ]\n",
    "\n",
    "  (p | beam.Create(persons)\n",
    "     | beam.ParDo(EnrichCountryDoFn(), cities_to_countries)\n",
    "     | beam.Map(print) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a647bcd2-9789-4094-bb2a-42f3c929c005",
   "metadata": {},
   "source": [
    "# Schemas for programming language types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8286aa99-6a98-4be4-b4ed-094376e8019e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "class Transaction(typing.NamedTuple):\n",
    "  bank: str\n",
    "  purchase_amount: float\n",
    "\n",
    "class ShippingAddress(typing.NamedTuple):\n",
    "  street_address: str\n",
    "  city: str\n",
    "  state: typing.Optional[str]\n",
    "  country: str\n",
    "  postal_code: str\n",
    "\n",
    "class Purchase(typing.NamedTuple):\n",
    "  user_id: str  # The id of the user who made the purchase.\n",
    "  item_id: int  # The identifier of the item that was purchased.\n",
    "  shipping_address: ShippingAddress  # The shipping address, a nested type.\n",
    "  cost_cents: int  # The cost of the item\n",
    "  transactions: typing.Sequence[Transaction]  # The transactions that paid for this purchase (a list, since the purchase might be spread out over multiple credit cards)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8785712-99f3-4cb0-98d0-7ebaafc5c7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d197aeb7-58d8-415b-9640-a6d17e3c8288",
   "metadata": {},
   "source": [
    "# Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efc4899-32e5-4c18-9d71-b011fe23ce05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "def partition_fn(word, num_partitions):\n",
    "    if word.isupper():  \n",
    "      return 0\n",
    "    elif word[0].isupper():\n",
    "      return 1\n",
    "    else:\n",
    "      return 2    \n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    parts = ( p | beam.io.ReadFromText('/home/sanvir/Desktop/kinglear.txt')\n",
    "            | beam.combiners.Sample.FixedSizeGlobally(1)\n",
    "            | beam.FlatMap(lambda line: line)\n",
    "            | beam.FlatMap(lambda sentence: sentence.split()))\n",
    "\n",
    "    result = ( parts | beam.Partition(partition_fn,3) )  \n",
    "  \n",
    "    allLetterUpperCase = result[0] \n",
    "    firstLetterUpperCase = result[1]\n",
    "    allLetterLowerCase = result[2]\n",
    "    \n",
    "    (allLetterUpperCase | \"Lower\" >> beam.Map(print))\n",
    "    \n",
    "    (firstLetterUpperCase | \"First\" >> beam.Map(print))\n",
    "    \n",
    "    (allLetterLowerCase | \"Upper\" >> beam.Map(print))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8053d7ac-8b13-43c3-b5b6-816ada8f7898",
   "metadata": {},
   "source": [
    "# Windowing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e94f7ce-f294-4ed4-bbec-9731d1ec90a0",
   "metadata": {},
   "source": [
    "## Example GlobalWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ac631-7693-4c05-9665-94dcf9337f55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam import window\n",
    "\n",
    "# Output PCollection\n",
    "class Output(beam.PTransform):\n",
    "    class _OutputFn(beam.DoFn):\n",
    "        def __init__(self, prefix=''):\n",
    "            super().__init__()\n",
    "            self.prefix = prefix\n",
    "\n",
    "        def process(self, element):\n",
    "            print(self.prefix+str(element))\n",
    "\n",
    "    def __init__(self, label=None,prefix=''):\n",
    "        super().__init__(label)\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def expand(self, input):\n",
    "        input | beam.ParDo(self._OutputFn(self.prefix))\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  (p | beam.Create(['Hello Beam','It`s windowing'])\n",
    "     | 'window' >>  beam.WindowInto(window.GlobalWindows())\n",
    "     | 'Log words' >> Output())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77692006-7e76-4d70-9446-3148426548c1",
   "metadata": {},
   "source": [
    "## Windowing X seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc553d7-97f2-4086-bfe0-fd01d61f8c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam import window\n",
    "\n",
    "# Output PCollection\n",
    "class Output(beam.PTransform):\n",
    "    class _OutputFn(beam.DoFn):\n",
    "        def __init__(self, prefix=''):\n",
    "            super().__init__()\n",
    "            self.prefix = prefix\n",
    "\n",
    "        def process(self, element):\n",
    "            print(self.prefix+str(element))\n",
    "\n",
    "    def __init__(self, label=None,prefix=''):\n",
    "        super().__init__(label)\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def expand(self, input):\n",
    "        input | beam.ParDo(self._OutputFn(self.prefix))\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  (p | beam.Create(['Hello Beam','It`s windowing'])\n",
    "     | 'window' >> beam.WindowInto(window.FixedWindows(60))\n",
    "     | 'Log words' >> Output())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4069d222-35c9-4b1d-927e-4186f5f8293c",
   "metadata": {},
   "source": [
    "## Windowing sliding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca1e9b6-0270-4ab9-98ee-7ad5d8e53008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following example code shows how to apply Window to divide a PCollection into sliding time windows. \n",
    "# Each window is 30 seconds in length, and a new window begins every five seconds:\n",
    "\n",
    "from apache_beam import window\n",
    "\n",
    "sliding_windowed_items = (\n",
    "    input | 'window' >> beam.WindowInto(window.SlidingWindows(30, 5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5852c9b-cf43-46a2-adf4-81b8e3ec8899",
   "metadata": {},
   "source": [
    "## Window Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4964b0f8-a55b-4b26-8022-4159b9c37cc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The following example code shows how to apply Window to divide a PCollection into session windows, where each session must be separated by a time gap of at least 10 minutes (600 seconds):\n",
    "from apache_beam import window\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  (p | beam.Create(['Hello Beam','It`s windowing'])\n",
    "     | 'window' >>  beam.WindowInto(window.Sessions(10 * 60))\n",
    "     | 'Log words' >> beam.Map(print))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252dfd2-93f8-430a-82a3-b8759ea28971",
   "metadata": {},
   "source": [
    "# Triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d66c7d-29fc-4925-9007-85a254c1d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set triggers for your PCollections to change this default behavior. Beam provides several pre-built triggers \n",
    "#    •Event time triggers. These triggers operate on the event time, as indicated by the timestamp on each data element. Beam’s default trigger is event time-based.\n",
    "#    •Processing time triggers. These triggers operate on the processing time – the time when the data element is processed at any given stage in the pipeline.\n",
    "#    •Data-driven triggers. These triggers operate by examining the data as it arrives in each window and firing when that data meets a certain property. \n",
    "# Currently, data-driven triggers only support firing after a certain number of data elements.\n",
    "#    •Composite triggers. These triggers combine multiple triggers in various ways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee703c8-c8bf-4a86-8dfa-fc02c84d6701",
   "metadata": {},
   "source": [
    "## Handling late data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee4dcb6-f523-4130-9bef-7e2f051c641a",
   "metadata": {},
   "source": [
    "# If you want your pipeline to process data that arrives after the watermark passes the end \n",
    "# of the window, you can apply an allowed lateness when you set your windowing configuration.\n",
    "# This gives your trigger the opportunity to react to the late data. If allowed lateness\n",
    "# is set, the default trigger will emit new results immediately whenever late data arrives.7\n",
    "# You set the allowed lateness by using .withAllowedLateness() when you set your windowing\n",
    "# function:input = [Initial PCollection]\n",
    "input | beam.WindowInto(\n",
    "            FixedWindows(60),\n",
    "            trigger=AfterProcessingTime(60),\n",
    "            allowed_lateness=1800) # 30 minutes\n",
    "     | ...\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130f674f-686d-4209-b5ba-1700d4ca25e5",
   "metadata": {},
   "source": [
    "## Types de built-in TRIGGERS:\n",
    "- AfterAll: It fires when all sub-triggers defined through of(List<Trigger> triggers) method are ready.\n",
    "- AfterEach: Are defined in inOrder(List<Trigger> triggers) method.The sub-trigger are executed in order, one by one.\n",
    "- AfterFirst: Executes when at least one of defined sub-triggers fires.\n",
    "- AfterPane: It uses elementCountAtLeast(int countElems).\n",
    "- AfterProcessingTime: Processing time-based trigger.\n",
    "- AfterWatermark: Its method pastEndOfWindow() creates a trigger firing the pane after the end of the window.\n",
    "- DefaultTrigger: It's the class used by default that is an equivalent to repeatable execution of AfterWatermak trigger.\n",
    "- NeverTrigger: The pane if fired only after the passes window plus allowed lateness delay.\n",
    "- OrFinallyTrigger: Special Trigger constructed througth Trigger's orFinally(OnceTrigger until method)\n",
    "- Repeatedly: To execute given trigger repeatedly. The sub-trigger is defined in forever(Trigger repeatable) method.\n",
    "    \n",
    "### Window accumulation\n",
    "    \n",
    "When you specify a trigger, you must also set the window's accumulation mode.\n",
    "\n",
    "To set a window to accumulate the panes that are produced when the trigger fires, invoke.accumulatingFiredPanes() when you set the trigger. To set a window to discard fired panes, invoke .discardingFiredPanes().\n",
    "    \n",
    "#### Accumulating mode\n",
    "    \n",
    "First trigger firing:  [5, 8, 3]\n",
    "Second trigger firing: [5, 8, 3, 15, 19, 23]\n",
    "Third trigger firing:  [5, 8, 3, 15, 19, 23, 9, 13, 10]\n",
    "    \n",
    "    ```\n",
    "    input | WindowInto(\n",
    "    FixedWindows(1 * 60),\n",
    "    trigger=AfterProcessingTime(1 * 60),\n",
    "    accumulation_mode=AccumulationMode.ACCUMULATING)\n",
    "    ```\n",
    "\n",
    "#### Discarding mode\n",
    "\n",
    "First trigger firing:  [5, 8, 3]\n",
    "Second trigger firing:           [15, 19, 23]\n",
    "Third trigger firing:                         [9, 13, 10]\n",
    "  \n",
    "    ```\n",
    "    input | WindowInto(\n",
    "    FixedWindows(1 * 60),\n",
    "    trigger=AfterProcessingTime(1 * 60),\n",
    "    accumulation_mode=AccumulationMode.DISCARDING)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae2454d-08a3-4e18-889f-afd0e458f189",
   "metadata": {},
   "source": [
    "# Event time trigger\n",
    "Is a trigger in Apache Beam that fires based on the timestamps of the elements in a pipeline,\n",
    "as opposed to the current processing time.\n",
    "\n",
    "## Modes of this type are\n",
    "\n",
    "### Discarding\n",
    "Any late data is discaded and only the data that arrives before the trigger fires is processed.\n",
    "\n",
    "#### Accumulating\n",
    "Late data is included and the trigger fires whenever the trigger condition are meet.\n",
    "\n",
    "```\n",
    "(p | beam.Create(['Hello Beam','It`s trigger'])\n",
    "   | 'window' >>  beam.WindowInto(FixedWindows(2),\n",
    "                                                trigger=trigger.AfterWatermark(early=trigger.AfterCount(2)),\n",
    "                                                accumulation_mode=trigger.AccumulationMode.DISCARDING,\n",
    "                                                timestamp_combiner=trigger.TimestampCombiner.OUTPUT_AT_EOW) \\\n",
    "   | ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c869a641-d38c-425d-92f9-21ed1f635ecd",
   "metadata": {},
   "source": [
    "##  Data driven trigger\n",
    "\n",
    "Fires when specific conditions on the data being processed are met.\n",
    "\n",
    "Apache Beam provides several options for data-driven triggers, including element\n",
    "count triggers, processing time triggers, and custom triggers.\n",
    "\n",
    "```\n",
    "(p | beam.Create(['Hello Beam','It`s trigger'])\n",
    "| 'window' >> beam.WindowInto(FixedWindows(2),trigger=trigger.AfterCount(2),accumulation_mode=trigger.AccumulationMode.DISCARDING) \\\n",
    "| ... )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eab4a19-af43-4a59-b0fe-a3b4506fc949",
   "metadata": {},
   "source": [
    "## Processing trigger\n",
    "\n",
    "Processing time trigger is a trigger in Apache Beam that fires based on the current processing time of the pipeline.\n",
    "\n",
    "```\n",
    "(p | beam.Create(['Hello Beam','It`s trigger'])\n",
    "   | 'window' >>  beam.WindowInto(FixedWindows(2),\n",
    "                                                trigger=trigger.AfterProcessingTime(1),\n",
    "                                                accumulation_mode=trigger.AccumulationMode.DISCARDING) \\\n",
    "   | ...)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51394bc0-d329-4bb9-9200-ddba25d2a376",
   "metadata": {},
   "source": [
    "## Composite trigger\n",
    "\n",
    "Allows to specify multiple triggers to be used in combination. when any of the trigger fire, the composite trigger will fire.\n",
    "\n",
    "```\n",
    "processing_time_trigger = trigger.AfterProcessingTime(60)\n",
    "event_time_trigger = trigger.AfterWatermark(early=trigger.AfterCount(100),\n",
    "                                             late=trigger.AfterCount(200))\n",
    "\n",
    "composite_trigger = trigger.AfterAll(processing_time_trigger,event_time_trigger)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af6b6c1-5322-4afa-afbb-ee8dc362e6f9",
   "metadata": {},
   "source": [
    "# Triggers Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3e93da-2f39-4175-9ffd-17490f095e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#   Licensed to the Apache Software Foundation (ASF) under one\n",
    "import apache_beam as beam\n",
    "\n",
    "# Output PCollection\n",
    "class Output(beam.PTransform):\n",
    "    class _OutputFn(beam.DoFn):\n",
    "        def __init__(self, prefix=''):\n",
    "            super().__init__()\n",
    "            self.prefix = prefix\n",
    "\n",
    "        def process(self, element):\n",
    "            print(self.prefix+str(element))\n",
    "\n",
    "    def __init__(self, label=None,prefix=''):\n",
    "        super().__init__(label)\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def expand(self, input):\n",
    "        input | beam.ParDo(self._OutputFn(self.prefix))\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    processing_time_trigger = beam.trigger.AfterProcessingTime(60)\n",
    "    # Define an event time trigger\n",
    "    event_time_trigger = beam.trigger.AfterWatermark(early=beam.trigger.AfterCount(100),\n",
    "                                             late=beam.trigger.AfterCount(200))\n",
    "\n",
    "    # Combine the processing time and event time triggers using the Or method\n",
    "    composite_trigger = beam.trigger.AfterAll(processing_time_trigger,event_time_trigger)\n",
    "\n",
    "    (p | beam.Create(['Hello Beam','It`s trigger','this','is','a','sample'])\n",
    "     | 'window' >>  beam.WindowInto(beam.window.FixedWindows(2),\n",
    "                                                trigger=composite_trigger ,\n",
    "                                                accumulation_mode=beam.trigger.AccumulationMode.DISCARDING)\n",
    "     | 'Log words' >> Output())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2ebaa4-5bba-45a7-95dc-dfe9e9b4c834",
   "metadata": {},
   "source": [
    "# TextIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbebe1b7-e905-4281-b79c-56108d38748a",
   "metadata": {},
   "source": [
    "## Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709f3fd1-5d46-4ea9-84ba-7cacab6dc2b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This provide a way to read and write text files in a pipeline.\n",
    "# We can use this same way to read GCS, S3, and HDFS\n",
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "p = beam.Pipeline()\n",
    "lines = p | beam.io.ReadFromText('/home/sanvir/Desktop/kinglear.txt')\n",
    "lines | beam.Map(print)\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d373bb-5927-47eb-bdbb-05c7b7af1f17",
   "metadata": {},
   "source": [
    "## Write file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bb6f998-5d86-4af1-be5e-c29fb834710f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.portability.fn_api_runner.fn_runner.RunnerResult at 0x7fdd48056580>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This provide a way to read and write text files in a pipeline.\n",
    "# We can use this same way to write GCS, S3, and HDFS\n",
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "p = beam.Pipeline()\n",
    "data = ['Hello, World!', 'Apache Beam']\n",
    "p | beam.Create(data) | beam.io.WriteToText('/home/sanvir/Desktop/samplefile.txt')\n",
    "p.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f10ca4c-c863-4a6f-b48b-e4fd5b950d9b",
   "metadata": {},
   "source": [
    "# TexIO GCS write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fed487-6e24-4b6f-b1b6-997906334308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to set up authentication to use GCS\n",
    "\n",
    "options = PipelineOptions()\n",
    "google_cloud_options = options.view_as(GoogleCloudOptions)\n",
    "google_cloud_options.project = 'my-project-id'\n",
    "google_cloud_options.job_name = 'myjob'\n",
    "google_cloud_options.staging_location = 'gs://my-bucket/staging'\n",
    "google_cloud_options.temp_location = 'gs://my-bucket/temp'\n",
    "google_cloud_options.region = 'us-central1'\n",
    "\n",
    "# set credentials\n",
    "credentials = GoogleCredentials.get_application_default()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f82f10-c2a5-4db5-8b9f-4611c1d08f7d",
   "metadata": {},
   "source": [
    "# BigQueryIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241eb197-2fbf-4774-b7d9-ee399e87abb3",
   "metadata": {},
   "source": [
    "## Reading BigQuery table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb6b3e3-9f8a-45a9-84ff-62d665318409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BigQueryIO allows you to read from a BigQuery table and read the results. \n",
    "# By default, Beam invokes a BigQuery export request when you apply a BigQueryIO read transform. \n",
    "# In Java Beam SDK, readTableRows returns a PCollection of BigQuery TableRow objects. \n",
    "# Each element in the PCollection represents a single row in the table.\n",
    "\n",
    "\n",
    "```\n",
    "p | 'ReadFromBigQuery' >> beam.io.ReadFromBigQuery(table='apache-beam-testing:clouddataflow_samples.weather_stations',\n",
    "                                                            method=beam.io.ReadFromBigQuery.Method.DIRECT_READ)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91e93b3-a7fd-4b27-a6a5-b47b689d4838",
   "metadata": {},
   "source": [
    "## Reading BigQuery query results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5db3aa-714e-4896-b1f7-b0ce6d6c8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "```\n",
    "lines = p | 'ReadFromBigQuery' >> beam.io.Read(beam.io.BigQuerySource(query='SELECT max_temperature FROM `tess-372508.fir.xasw`'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d47798e-ba68-42c0-b35c-08b545e401af",
   "metadata": {},
   "source": [
    "## BigQueryIO write table-schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de81c684-d04e-4297-b339-b424b69b3293",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    You can use the dynamic destinations feature to write elements in a PCollection to different BigQuery tables, \n",
    "    possibly with different schemas.The dynamic destinations feature groups your user type by a user-defined\n",
    "    destination key, uses the key to compute a destination table and/or schema, and writes each group’s elements\n",
    "    to the computed destination.In addition, you can also write your own types that have a mapping function to TableRow, \n",
    "    and you can use side inputs in all DynamicDestinations methods.\n",
    "\"\"\"\n",
    "\n",
    "```\n",
    "fictional_characters_view = beam.pvalue.AsDict(\n",
    "    pipeline | 'CreateCharacters' >> beam.Create([('Yoda', True),\n",
    "                                                  ('Obi Wan Kenobi', True)]))\n",
    "\n",
    "def table_fn(element, fictional_characters):\n",
    "  if element in fictional_characters:\n",
    "    return 'my_dataset.fictional_quotes'\n",
    "  else:\n",
    "    return 'my_dataset.real_quotes'\n",
    "\n",
    "quotes | 'WriteWithDynamicDestination' >> beam.io.WriteToBigQuery(\n",
    "    table_fn,\n",
    "    schema=table_schema,\n",
    "    table_side_inputs=(fictional_characters_view, ),\n",
    "    write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE,\n",
    "    create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68680681-76da-4db4-91ba-9220d4c43455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "p = beam.Pipeline()\n",
    "\n",
    "table_spec = bigquery.TableReference(\n",
    "                 projectId='project-id',\n",
    "                 datasetId='dataset',\n",
    "                 tableId='table')\n",
    "\n",
    "table_schema = {\n",
    "    'fields': [{\n",
    "        'name': 'source', 'type': 'STRING', 'mode': 'NULLABLE'\n",
    "    }, {\n",
    "        'name': 'quote', 'type': 'STRING', 'mode': 'REQUIRED'\n",
    "    }]\n",
    "}\n",
    "\n",
    "input = p | beam.Create([\n",
    "    {\n",
    "        'source': 'Mahatma Gandhi', 'quote': 'My life is my message.'\n",
    "    },\n",
    "    {\n",
    "        'source': 'Yoda', 'quote': \"Do, or do not. There is no 'try'.\"\n",
    "    },\n",
    "])\n",
    "\n",
    "# It defines the schema (table_schema) of the table. The table has two fields: source and quote, both of type STRING.\n",
    "# The source field is nullable, while the quote field is required.\n",
    "\n",
    "# It creates the input data which is a collection of dictionaries. Each dictionary represents a row in the BigQuery table.\n",
    "\n",
    "# Finally, it writes the data to the BigQuery table using the beam.io.WriteToBigQuery function. The write_disposition\n",
    "# parameter is set to WRITE_TRUNCATE which means that if the table already exists, it will be replaced with the new\n",
    "# data. The create_disposition parameter is set to CREATE_IF_NEEDED which means the table will be created if it does\n",
    "# not exist.\n",
    "\n",
    "input | beam.io.WriteToBigQuery(\n",
    "    table_spec,\n",
    "    schema=table_schema,\n",
    "    write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE,\n",
    "    create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89613c8-4a6c-4574-b623-251ee3f4b9bb",
   "metadata": {},
   "source": [
    "## KafkaIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe05cd93-744c-4646-96f3-c96dca39a9c7",
   "metadata": {},
   "source": [
    "### Read\n",
    "\n",
    "When reading data from Kafka topics using Apache Beam, developers can use the ReadFromKafka transform to create a PCollection of Kafka messages. \n",
    "This transform takes the following parameters:\n",
    "•consumer_config: a dictionary that contains the Kafka consumer configuration properties, such as the Kafka broker addresses, the group ID of the consumer group, and the deserializer classes for the key and value of the Kafka messages.\n",
    "•bootstrap.servers: is a configuration property in Apache Kafka that specifies the list of bootstrap servers that the Kafka clients should use to connect to the Kafka cluster.\n",
    "•topic: the name of the Kafka topic to write the data to.\n",
    "•with_metadata: a boolean flag that specifies whether to include the Kafka metadata for each message, such as the topic, partition, and offset.For detailed information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38f4a05-d678-4001-8721-6f2e41e5971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_topic = 'input-topic'\n",
    "output_topic = 'output-topic'\n",
    "\n",
    "(p | \"Read from Kafka\" >> ReadFromKafka(\n",
    "      topics=[input_topic],\n",
    "      bootstrap_servers='localhost:9092')\n",
    " | \"Process data\" >> beam.Map(process_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ce04f-03de-48c3-8ce0-a57eba046175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "def process_data(element):\n",
    "    # Do some processing on the data\n",
    "    return element\n",
    "\n",
    "options = beam.options.pipeline_options.PipelineOptions()\n",
    "p = beam.Pipeline(options=options)\n",
    "\n",
    "input_topic = 'input-topic'\n",
    "output_topic = 'output-topic'\n",
    "\n",
    "bootstrap_servers = {\"bootstrap.servers\": \"kafka_server:9092\"}\n",
    "\n",
    "# Set Kafka parameters: The Kafka topic to read from (input_topic), the Kafka topic to write to (output_topic),\n",
    "# and the Kafka brokers to connect to (bootstrap_servers) are specified.\n",
    "\n",
    "# Read from Kafka topic: A KafkaIO ReadFromKafka transform is created, where the topics method is used to specify the\n",
    "# Kafka topic to read from and the consumer_config method is used to specify the Kafka brokers to connect to.\n",
    "\n",
    "# Process the data: The data read from Kafka is processed using the beam.Map(process_data) method. In this case,\n",
    "# the data is simply passed to the process_data function defined earlier.\n",
    "\n",
    "\n",
    "\n",
    "# (p | \"Read from Kafka\" >> ReadFromKafka(\n",
    "#       topics=[input_topic],\n",
    "#       consumer_config=bootstrap_servers)\n",
    "#  | \"Process data\" >> beam.Map(process_data))\n",
    "\n",
    "p.run().wait_until_finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1285af-0762-44de-8ab4-96510b361f5a",
   "metadata": {},
   "source": [
    "## Write"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85206bc9-45a2-4c38-ab37-5f5c2e131092",
   "metadata": {},
   "source": [
    "To use the WriteToKafka transform, developers need to provide the following parameters:\n",
    "•producer_config: a dictionary that contains the Kafka producer configuration properties, \n",
    "such as the Kafka broker addresses and the number of acknowledgments to wait for before \n",
    "considering a message as sent.\n",
    "•bootstrap.servers: is a configuration property in Apache Kafka that specifies the list\n",
    "of bootstrap servers that the Kafka clients should use to connect to the Kafka cluster.\n",
    "•topic: the name of the Kafka topic to write the data to.\n",
    "•key: a function that takes an element from the input PCollection and returns the key to use\n",
    "for the Kafka message. The key is optional and can be None.\n",
    "•value: a function that takes an element from the input PCollection and returns the value \n",
    "to use for the Kafka message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a731c2-aa59-428a-a81a-5c84d42be6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(input |  \"Write to Kafka\" >> WriteToKafka(\n",
    "       topic=output_topic,\n",
    "       producer_config = bootstrap_servers,\n",
    "       key='key',\n",
    "       value='value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde22ab-72eb-4c56-aff8-b953bbfc9ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "def process_data(element):\n",
    "    # Do some processing on the data\n",
    "    return element\n",
    "\n",
    "options = beam.options.pipeline_options.PipelineOptions()\n",
    "p = beam.Pipeline(options=options)\n",
    "\n",
    "input_topic = 'input-topic'\n",
    "output_topic = 'output-topic'\n",
    "bootstrap_servers = {\"bootstrap.servers\": \"kafka_server:9092\"}\n",
    "\n",
    "input = p | beam.Create([{\"key\": \"foo\", \"value\": \"bar\"}])\n",
    "\n",
    "\"\"\"\n",
    "This pipeline is an example of how you can use Apache Beam's KafkaIO (in Python SDK) to write data to a Kafka \n",
    "topic.Make sure your Kafka server is accessible and running, and the topic exists. \n",
    "\"\"\"\n",
    "\n",
    "# (input |  \"Write to Kafka\" >> WriteToKafka(\n",
    "#       topic=output_topic,\n",
    "#       producer_config = bootstrap_servers)\n",
    "# )\n",
    "\n",
    "p.run().wait_until_finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497a6bad-2d4d-4a4a-a581-073d0c954f3f",
   "metadata": {},
   "source": [
    "# REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef4ff77-c66a-4a7d-892e-b1d6481cc1bb",
   "metadata": {},
   "source": [
    "```\n",
    "fictional_characters_view = beam.pvalue.AsDict(\n",
    "    pipeline | 'CreateCharacters' >> beam.Create([('Yoda', True),('Obi Wan Kenobi', True)]))\n",
    "\n",
    "def table_fn(element, fictional_characters):\n",
    "  if element in fictional_characters:\n",
    "    return 'my_dataset.fictional_quotes'\n",
    "  else:\n",
    "    return 'my_dataset.real_quotes'\n",
    "\n",
    "quotes | 'WriteWithDynamicDestination' >> beam.io.WriteToBigQuery(\n",
    "    table_fn,\n",
    "    schema=table_schema,\n",
    "    table_side_inputs=(fictional_characters_view, ),\n",
    "    write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE,\n",
    "    create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad29acd9-4f9d-4970-9dd3-b71fd4eb0dcc",
   "metadata": {},
   "source": [
    "Final Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d7f858-4f8e-4576-ad73-7054e6ce50a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import logging\n",
    "import re\n",
    "from apache_beam.transforms import window, trigger\n",
    "from apache_beam.transforms.combiners import CountCombineFn\n",
    "\n",
    "class Transaction:\n",
    "    def __init__(self, transaction_no, date, product_no, product_name, price, quantity, customer_no, country):\n",
    "        self.transaction_no = transaction_no\n",
    "        self.date = date\n",
    "        self.product_no = product_no\n",
    "        self.product_name = product_name\n",
    "        self.price = price\n",
    "        self.quantity = quantity\n",
    "        self.customer_no = customer_no\n",
    "        self.country = country\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Transaction(transaction_no={self.transaction_no}, date='{self.date}', product_no='{self.product_no}', product_name='{self.product_name}', price={self.price}, quantity={self.quantity}, customer_no={self.customer_no}, country='{self.country}')\"\n",
    "\n",
    "def run():\n",
    "    with beam.Pipeline() as pipeline:\n",
    "      transactions = (pipeline\n",
    "                        | 'Read from text file' >> beam.io.ReadFromText('input.csv')\n",
    "                        | 'window' >>  beam.WindowInto(beam.window.FixedWindows(30))\n",
    "                        | 'Filter' >>\n",
    "                     )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()\n",
    "\n",
    "\n",
    "# Define an event time trigger\n",
    "# event_time_trigger = beam.trigger.AfterWatermark(early=beam.trigger.AfterCount(100),\n",
    "                                         # late=beam.trigger.AfterCount(200))\n",
    "\n",
    "# Combine the processing time and event time triggers using the Or method\n",
    "composite_trigger = beam.trigger.AfterAll(processing_time_trigger,event_time_trigger)\n",
    "\n",
    "# (p | beam.Create(['Hello Beam','It`s trigger','this','is','a','sample'])\n",
    "#  | 'window' >>  beam.WindowInto(beam.window.FixedWindows(2),\n",
    "#                                             trigger=composite_trigger ,\n",
    "#                                             accumulation_mode=beam.trigger.AccumulationMode.DISCARDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0554f116-5b71-4881-b87b-395eb73a1c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00f29e7a-8276-4516-9d71-925c311c5a64",
   "metadata": {},
   "source": [
    "# FINAL CHALLENGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732b7572-07cc-41fb-b07e-cc2cccbd7f83",
   "metadata": {},
   "source": [
    "Final challenge 1You’re given a csv file with purchase transactions. \n",
    "Write a Beam pipeline to prepare a report every 30 seconds.\n",
    "The report needs to be created only for transactions where quantity is more than 20.\n",
    "Report should consist of two files named \"price_more_than_10.txt\" and \"price_less_than_10.txt\":\n",
    "•Total transactions amount grouped by ProductNo for products with price greater than 10\n",
    "•Total transactions amount grouped by ProductNo for products with price less than 10Example rows from input file:\n",
    "\n",
    "Sample of Data:\n",
    "581538,12/9/2019,22561,Wooden School Colouring Set,7.24,1,14446,United Kingdom     \n",
    "581538,12/9/2019,84519A,Tomato Charlie+Lola Coaster Set,7.24,1,14446,United Kingdom\n",
    "581538,12/9/2019,84519B,Carrot Charlie+Lola Coaster Set,7.24,1,14446,United Kingdom\n",
    "581538,12/9/2019,35004B,Set Of 3 Black Flying Ducks,7.24,2,14446,United Kingdom    \n",
    "581538,12/9/2019,22068,Black Pirate Treasure Chest,7.24,1,14446,United Kingdom     \n",
    "581538,12/9/2019,23353,6 Gift Tags Vintage Christmas,6.19,1,14446,United Kingdom   \n",
    "581538,12/9/2019,21591,Cosy Hour Cigar Box Matches,6.19,1,14446,United Kingdom     \n",
    "581538,12/9/2019,22197,Popcorn Holder,6.19,4,14446,United Kingdom                  \n",
    "581538,12/9/2019,23320,Giant 50'S Christmas Cracker,6.19,1,14446,United Kingdom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c8d9bb-c7ec-4824-819f-572874c31e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import logging\n",
    "import re\n",
    "from apache_beam.transforms import window, trigger\n",
    "from apache_beam.transforms.combiners import CountCombineFn\n",
    "\n",
    "class Transaction:\n",
    "    def __init__(self, transaction_no, date, product_no, product_name, price, quantity, customer_no, country):\n",
    "        self.transaction_no = transaction_no\n",
    "        self.date = date\n",
    "        self.product_no = product_no\n",
    "        self.product_name = product_name\n",
    "        self.price = price\n",
    "        self.quantity = quantity\n",
    "        self.customer_no = customer_no\n",
    "        self.country = country\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Transaction(transaction_no={self.transaction_no}, date='{self.date}', product_no='{self.product_no}', product_name='{self.product_name}', price={self.price}, quantity={self.quantity}, customer_no={self.customer_no}, country='{self.country}')\"\n",
    "\n",
    "\n",
    "class linesToObject(beam.DoFn):\n",
    "    def process(self, line):  \n",
    "        spl = line.split(',')        \n",
    "        if spl[0] !=  'TransactionNo' :    \n",
    "            yield  Transaction(spl[0],spl[1],spl[2],spl[3],spl[4],spl[5],spl[6],spl[7])    \n",
    "\n",
    "def partitonTrans(transaction):\n",
    "    if float(transaction.price) >= 10.0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1              \n",
    "    \n",
    "\n",
    "def run():\n",
    "    with beam.Pipeline() as pipeline:\n",
    "        transactions = (pipeline\n",
    "                        | 'Read from text file' >> beam.io.ReadFromText('input.csv'))\n",
    "\n",
    "\n",
    "        transOb = (transactions | 'ConvertLinesToTransactionObject' >> beam.ParDo(linesToObject()))\n",
    "\n",
    "        windowed_transactions = (transOb | 'Window' >> beam.WindowInto(window.FixedWindows(30), trigger=trigger.AfterWatermark( early=trigger.AfterProcessingTime(5).has_ontime_pane(), late=trigger.AfterAll()), allowed_lateness=30, accumulation_mode=trigger.AccumulationMode.DISCARDING))                \n",
    "\n",
    "        filter20 = (transOb | 'Quantity>20' >>beam.Filter(lambda t: int(t.quantity) > 20))\n",
    "\n",
    "        groups = (filter20  | 'partitions' >> beam.Partition(partitionTrans, 2))  \n",
    "\n",
    "        priceGreater10 = groups[0]    \n",
    "        priceLower10 = groups[1]           \n",
    "\n",
    "        (priceGreater10 \n",
    "                  | 'Dict>10' >> beam.Map(lambda t: (t.transaction_no, float(t.price)))\n",
    "                  | 'SumByKey>10' >> beam.CombinePerKey(sum) \n",
    "                  | 'Save' >> beam.io.WriteToText('price_more_than_10.txt'))\n",
    "\n",
    "        (priceLower10 \n",
    "                  | 'Dict>10' >> beam.Map(lambda t: (t.transaction_no, float(t.price)))\n",
    "                  | 'SumByKey<10' >> beam.CombinePerKey(sum) \n",
    "                  | 'Save' >> beam.io.WriteToText('price_less_than_10.txt'))\n",
    "\n",
    "if __name__ == '__main__':  \n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1545d82-c223-43fc-b2a1-b2597a223bc2",
   "metadata": {},
   "source": [
    "# FINAL CHALLENGE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a798a82e-4130-4ea1-acbc-1c0702765fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final challenge 2You are given a file analyzed.csv which maps words to sentiments. \n",
    "Using this, analyze kinglear.txt.\n",
    "Output PCollections counting the number of negative words and positive words as well as PCollections counting the number of positive words with strong modal and positive words with weak modal.\n",
    "Example rows from input file:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6365bb-10a0-4d0d-af63-18e91300149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class SplitWords(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        word = element.upper().split(\" \")      \n",
    "        return word  \n",
    "        \n",
    "class convertToAnalysis(beam.DoFn):\n",
    "    def process(self, element):\n",
    "      sp =  element.split(',')\n",
    "      if sp[0] != 'Word':\n",
    "        yield Analysis(sp[0],sp[1],sp[2],sp[3],sp[4],sp[5],sp[6],sp[7])\n",
    "    \n",
    "class CountFn(beam.CombineFn):\n",
    "    def create_accumulator(self):\n",
    "        return 0\n",
    "\n",
    "    def add_input(self, accumulator, element):\n",
    "        count = accumulator\n",
    "        return count + 1\n",
    "\n",
    "    def merge_accumulators(self, accumulators):\n",
    "        counts = accumulators\n",
    "        return sum(counts)\n",
    "\n",
    "    def extract_output(self, accumulator):\n",
    "        (count) = accumulator\n",
    "        return count if count else 0\n",
    "                                    \n",
    "class Analysis:\n",
    "    def __init__(self, word, negative, positive, uncertainty, litigious, strong, weak, constraining):\n",
    "        self.word = word\n",
    "        self.negative = negative\n",
    "        self.positive = positive\n",
    "        self.uncertainty = uncertainty\n",
    "        self.litigious = litigious\n",
    "        self.strong = strong\n",
    "        self.weak = weak\n",
    "        self.constraining = constraining\n",
    "\n",
    "    def __str__(self):\n",
    "        return (f'Analysis(word={self.word}, negative={self.negative}, positive={self.positive}, '\n",
    "                f'uncertainty={self.uncertainty}, litigious={self.litigious}, strong={self.strong}, '\n",
    "                f'weak={self.weak}, constraining={self.constraining})')\n",
    "\n",
    "def remove_non_letters(word):\n",
    "    \"\"\"Removes non-letter characters from a word.\n",
    "\n",
    "    Args:\n",
    "      word: The word to remove non-letters from.\n",
    "\n",
    "    Returns:\n",
    "      The word with non-letter characters removed.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"[^a-zA-Z]+\", \"\", word)\n",
    "\n",
    "class match(beam.DoFn):\n",
    "  def process(self, element, dataset):\n",
    "       if element.word in dataset:\n",
    "          yield element\n",
    "     \n",
    "\n",
    "def run():\n",
    "    pipeline_options = PipelineOptions()\n",
    "    with beam.Pipeline(options=pipeline_options) as p:\n",
    "      kinglear_words = (p\n",
    "                       | 'Read from text file' >> ReadFromText('gs://apache-beam-samples/shakespeare/kinglear.txt')\n",
    "                       | 'Split into words' >> beam.ParDo(SplitWords())\n",
    "                       | 'Filter empty words' >> beam.Filter(bool)\n",
    "                       | 'Remove Non Letters' >> beam.Map(lambda word: remove_non_letters(word)) \n",
    "                       | 'Set' >> beam.Distinct() \n",
    "                       #| 'Print' >> beam.Map(print)\n",
    "                       )\n",
    "\n",
    "      sentiment_words = (p \n",
    "                       | 'Read sentiment words from csv' >> ReadFromText('analysis.csv')\n",
    "                       | 'CSV into words' >> beam.ParDo(convertToAnalysis())  ) \n",
    "                       #| 'Print' >> beam.Map(print) )      \n",
    "\n",
    "      positive_words = (sentiment_words\n",
    "                       | 'Positive Words' >> beam.Filter(lambda a: int(a.positive) > 0)\n",
    "                       #| 'Print positives' >> beam.Map(print)                            \n",
    "                       ) \n",
    "\n",
    "      negative_words = (sentiment_words\n",
    "                       | 'Negative Words' >> beam.Filter(lambda a: int(a.negative) > 0)\n",
    "                       #| 'Print negatives' >> beam.Map(print)       \n",
    "                       )                                \n",
    "                       \n",
    "      strong_modal = (positive_words\n",
    "                       | 'Strong Modal' >> beam.Filter(lambda a: int(a.strong) > 0)\n",
    "                       #| 'Print negatives' >> beam.Map(print)       \n",
    "                       )      \n",
    "       \n",
    "      weak_modal = (positive_words\n",
    "                       | 'Weak Modal' >> beam.Filter(lambda a: int(a.weak) > 0)\n",
    "                       #| 'Print negatives' >> beam.Map(print)       \n",
    "                       )   \n",
    "                       \n",
    "      inter_kinglear_positive = ( positive_words \n",
    "                       | 'Intersection Positive' >> beam.ParDo(match(), beam.pvalue.AsIter(kinglear_words)) \n",
    "                       #| 'Print match' >> beam.Map(print)\n",
    "                       )\n",
    "                       \n",
    "      inter_kinglear_negative = ( negative_words \n",
    "                       | 'Intersection Negative' >> beam.ParDo(match(), beam.pvalue.AsIter(kinglear_words)) \n",
    "                       #| 'Print match' >> beam.Map(print)\n",
    "                       )\n",
    "                       \n",
    "      inter_kinglear_strong = ( strong_modal \n",
    "                       | 'Intersection Strong' >> beam.ParDo(match(), beam.pvalue.AsIter(kinglear_words)) \n",
    "                       #| 'Print match' >> beam.Map(print)\n",
    "                       )\n",
    "                       \n",
    "      inter_kinglear_weak = ( weak_modal \n",
    "                       | 'Intersection Weak' >> beam.ParDo(match(), beam.pvalue.AsIter(kinglear_words)) \n",
    "                       #| 'Print match' >> beam.Map(print)\n",
    "                       )\n",
    "      \n",
    "      count_positives = ( inter_kinglear_positive\n",
    "                        | 'Count Positives' >>  beam.CombineGlobally(CountFn()).without_defaults()\n",
    "                        #| 'Print match' >> beam.Map(print)\n",
    "                        )\n",
    "                        \n",
    "      count_negatives = ( inter_kinglear_negative\n",
    "                        | 'Count Negatives' >>  beam.CombineGlobally(CountFn()).without_defaults()\n",
    "                        #| 'Print match' >> beam.Map(print)\n",
    "                        )\n",
    "                            \n",
    "      count_positives_strong = ( inter_kinglear_strong\n",
    "                        | 'Count Positives Strong' >>  beam.CombineGlobally(CountFn()).without_defaults()\n",
    "                        #| 'Print match' >> beam.Map(print)\n",
    "                        )  \n",
    "                        \n",
    "      count_positives_weak = ( inter_kinglear_weak\n",
    "                        | 'Count Positives Weak' >>  beam.CombineGlobally(CountFn()).without_defaults()\n",
    "                        | 'Print match' >> beam.Map(print)\n",
    "                        )                                                                  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b794469c-db55-4ece-9706-8e1a093a4042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c59a3a-f729-48f6-84fd-8625f972e802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6098090-0cce-4421-a2cf-612242692201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf17d7d5",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35504569-d00b-44d2-b7ac-ea1204a1d851",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(bank='Hola', purchase_amount=12, option=True)\n",
      "Row(bank='Saludos', purchase_amount=40, option=False)\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "class Transaction(typing.NamedTuple):\n",
    "  bank: str\n",
    "  purchase_amount: float\n",
    "    \n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    pc = (p \n",
    "          | beam.Create([{\"bank\":\"Hola\", \"purchase_amount\":12, \"option\":True},{\"bank\":\"Saludos\", \"purchase_amount\":40, \"option\":False}])\n",
    "          | beam.Map(lambda item: beam.Row(bank=item[\"bank\"],purchase_amount=item[\"purchase_amount\"], option=item[\"option\"]) )\n",
    "          | beam.Map(print)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf5ffd2-2000-44c9-b5fa-2c1b744d61cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache-beam",
   "language": "python",
   "name": "apache-beam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
